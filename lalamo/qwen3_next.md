The structure of qwen3_next from transformers is as follows:

```text
lm_head.weight
model.embed_tokens.weight
model.layers.<0-26>.input_layernorm.weight
model.layers.<0-26>.post_attention_layernorm.weight
model.layers.<0-26>.linear_attn.A_log
model.layers.<0-26>.linear_attn.conv1d.weight
model.layers.<0-26>.linear_attn.dt_bias
model.layers.<0-26>.linear_attn.in_proj_ba.weight
model.layers.<0-26>.linear_attn.in_proj_qkvz.weight
model.layers.<0-26>.linear_attn.norm.weight
model.layers.<0-26>.linear_attn.out_proj.weight
model.layers.<0-26>.self_attn.k_norm.weight
model.layers.<0-26>.self_attn.k_proj.weight
model.layers.<0-26>.self_attn.o_proj.weight
model.layers.<0-26>.self_attn.q_norm.weight
model.layers.<0-26>.self_attn.q_proj.weight
model.layers.<0-26>.self_attn.v_proj.weight
model.layers.<0-26>.mlp.gate.weight
model.layers.<0-26>.mlp.shared_expert_gate.weight
model.layers.<0-26>.mlp.shared_expert.down_proj.weight
model.layers.<0-26>.mlp.shared_expert.gate_proj.weight
model.layers.<0-26>.mlp.shared_expert.up_proj.weight
model.layers.<0-26>.mlp.experts.<0-511>.down_proj.weight
model.layers.<0-26>.mlp.experts.<0-511>.gate_proj.weight
model.layers.<0-26>.mlp.experts.<0-511>.up_proj.weight
model.norm.weight
```
